"""
Omnius FIXED PFC - Single response, proper streaming
"""
from typing import Dict, Any, Tuple, List, Optional, AsyncGenerator
import json
import re
import asyncio

from app.services.llm_service import llm_service
from app.services.deepseek_coder_service import deepseek_coder
from app.core.database import db


class OmniusFixedPFC:
    """Fixed PFC with proper synthesis and streaming"""
    
    def __init__(self):
        self.name = "OMNIUS"
        self.prefrontal_cortex = llm_service
        self.code_cortex = deepseek_coder
        self.is_streaming = False
        
    async def initialize(self, db_pool):
        """Initialize"""
        print("ðŸ§  Omnius FIXED PFC initialized")
        
    async def check_tokens(self, user_id: str) -> Dict:
        """Check tokens"""
        try:
            result = await db.fetchrow("""
                SELECT * FROM user_token_status WHERE user_id = $1::uuid
            """, user_id)
            
            if not result:
                await db.execute("""
                    INSERT INTO user_tokens (user_id) VALUES ($1::uuid)
                    ON CONFLICT (user_id) DO NOTHING
                """, user_id)
                return {'has_tokens': True, 'tokens_remaining': 1000}
            
            return {
                'has_tokens': result.get('tokens_remaining', 0) > 0,
                'tokens_remaining': result.get('tokens_remaining', 0)
            }
        except:
            return {'has_tokens': True, 'tokens_remaining': 1000}
    
    async def think_stream(self, message: str, context: Dict[str, Any]) -> AsyncGenerator[Dict, None]:
        """
        TRUE STREAMING response generation
        """
        self.is_streaming = True
        
        # Send initial thinking status
        yield {"type": "status", "content": "ðŸ§  Analyzing request..."}
        await asyncio.sleep(0.1)
        
        # Analyze
        print("\n" + "="*70)
        print("ðŸ§  [PFC ANALYSIS]")
        analysis = await self._quick_analysis(message)
        print(f"  Need code: {analysis['needs_code']}")
        print(f"  Need explanation: {analysis['needs_explanation']}")
        
        yield {"type": "status", "content": "ðŸ“‹ Planning approach..."}
        await asyncio.sleep(0.1)
        
        # Execute based on analysis
        if analysis['needs_code'] and analysis['needs_explanation']:
            # Both needed - do explanation first, then code
            yield {"type": "status", "content": "âœï¸ Generating explanation..."}
            
            # Stream explanation
            async for chunk in self._stream_explanation(message):
                yield {"type": "content", "chunk": chunk}
            
            yield {"type": "content", "chunk": "\n\n"}
            yield {"type": "status", "content": "ðŸ’» Generating optimized code..."}
            
            # Stream code
            async for chunk in self._stream_code(message):
                yield {"type": "content", "chunk": chunk}
                
        elif analysis['needs_code']:
            yield {"type": "status", "content": "ðŸ’» Generating code..."}
            async for chunk in self._stream_code(message):
                yield {"type": "content", "chunk": chunk}
                
        else:
            yield {"type": "status", "content": "âœï¸ Generating response..."}
            async for chunk in self._stream_explanation(message):
                yield {"type": "content", "chunk": chunk}
        
        # Send completion
        yield {"type": "complete", "content": "âœ… Response complete"}
        self.is_streaming = False
    
    async def think(self, message: str, context: Dict[str, Any]) -> Tuple[str, Dict]:
        """
        Non-streaming version for regular API calls
        """
        print("\n" + "="*70)
        print("ðŸ§  [OMNIUS PFC THINKING]")
        print("="*70)
        print(f"ðŸ“¥ Input: {message[:100]}...")
        
        # Quick analysis
        analysis = await self._quick_analysis(message)
        
        print(f"\nðŸ“Š Analysis:")
        print(f"  - Needs code: {analysis['needs_code']}")
        print(f"  - Needs explanation: {analysis['needs_explanation']}")
        print(f"  - Complexity: {analysis['complexity']}")
        
        # Execute based on needs (NO DUPLICATES)
        final_parts = []
        regions_used = ['prefrontal_cortex']
        
        if analysis['needs_explanation']:
            print("\nðŸ“ Generating explanation...")
            explanation = await self._generate_explanation(message, analysis)
            if explanation:
                final_parts.append(explanation)
                print(f"  âœ“ Explanation: {len(explanation)} chars")
        
        if analysis['needs_code']:
            print("\nðŸ’» Generating code...")
            code = await self._generate_code(message, analysis)
            if code:
                final_parts.append(code)
                regions_used.append('code_cortex')
                print(f"  âœ“ Code: {len(code)} chars")
        
        # Single clean response
        final_response = "\n\n".join(final_parts)
        
        print(f"\nâœ… Final response: {len(final_response)} chars")
        print("="*70)
        
        return final_response, {
            'consciousness_used': regions_used,
            'neurochemistry_active': False,
            'streaming_capable': True
        }
    
    async def _quick_analysis(self, message: str) -> Dict:
        """
        Quick heuristic analysis (can be replaced with LLM call for better accuracy)
        """
        msg_lower = message.lower()
        
        # Determine needs
        needs_code = any(word in msg_lower for word in [
            'function', 'code', 'implement', 'create', 'write', 'program',
            'algorithm', 'class', 'method', 'script'
        ])
        
        needs_explanation = any(word in msg_lower for word in [
            'explain', 'how', 'what', 'why', 'describe', 'tell'
        ]) or 'prime' in msg_lower  # Special case for prime numbers
        
        # Complexity assessment
        complexity = 5
        if 'optimize' in msg_lower or 'efficient' in msg_lower:
            complexity = 8
        elif 'simple' in msg_lower or 'basic' in msg_lower:
            complexity = 3
        
        return {
            'needs_code': needs_code,
            'needs_explanation': needs_explanation or not needs_code,  # Default to explanation if nothing else
            'complexity': complexity
        }
    
    async def _generate_explanation(self, message: str, analysis: Dict) -> str:
        """
        Generate explanation part
        """
        if 'prime' in message.lower() and analysis['needs_code']:
            # Special handling for prime numbers with code
            prompt = f"""[INST]Briefly explain the approach for: {message}
Focus on the algorithm choice and optimization strategy.
Keep it concise - the code will follow.[/INST]"""
        else:
            prompt = f"""[INST]Explain clearly: {message}[/INST]"""
        
        return self.prefrontal_cortex.generate(
            prompt,
            temperature=0.7,
            max_tokens=500
        )
    
    async def _generate_code(self, message: str, analysis: Dict) -> str:
        """
        Generate code part
        """
        if analysis['complexity'] > 7:
            prompt = f"Create optimized, efficient implementation. {message}"
        else:
            prompt = message
        
        code = deepseek_coder.generate_code(prompt)
        
        # Ensure proper formatting
        if '```' not in code:
            code = f"```python\n{code}\n```"
        
        return code
    
    async def _stream_explanation(self, message: str) -> AsyncGenerator[str, None]:
        """
        Stream explanation in chunks
        """
        explanation = self.prefrontal_cortex.generate(
            f"[INST]Explain: {message}[/INST]",
            temperature=0.7,
            max_tokens=500
        )
        
        # Simulate streaming by chunking
        chunk_size = 30
        for i in range(0, len(explanation), chunk_size):
            chunk = explanation[i:i+chunk_size]
            yield chunk
            await asyncio.sleep(0.02)  # Small delay for streaming effect
    
    async def _stream_code(self, message: str) -> AsyncGenerator[str, None]:
        """
        Stream code in chunks
        """
        code = deepseek_coder.generate_code(message)
        
        if '```' not in code:
            code = f"```python\n{code}\n```"
        
        # Stream in chunks
        chunk_size = 50
        for i in range(0, len(code), chunk_size):
            chunk = code[i:i+chunk_size]
            yield chunk
            await asyncio.sleep(0.01)
    
    def get_status(self) -> Dict:
        """Get status"""
        return {
            'prefrontal_cortex': 'active',
            'code_cortex': 'active',
            'math_region': 'not_installed',
            'creative_center': 'not_installed',
            'neurochemistry': 'ready_but_disabled',
            'total_parameters': '~14B',
            'active_regions': 2,
            'processing_power': 0.7,
            'streaming_enabled': True
        }
    
    async def shutdown(self):
        """Shutdown"""
        print("ðŸ‘‹ Omnius Fixed PFC shutdown")


# Create instance
omnius_neurochemical = OmniusFixedPFC()
